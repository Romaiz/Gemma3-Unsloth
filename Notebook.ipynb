{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3285793",
   "metadata": {},
   "source": [
    "SETUP AND INITAL CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c69acc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Romaiz\\Documents\\Python_ML\\RL\\Fine_Tuning\\Gemme3-Unsloth\\.venv\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find cuobjdump.exe\n",
      "  warnings.warn(f\"Failed to find {binary}\")\n",
      "c:\\Users\\Romaiz\\Documents\\Python_ML\\RL\\Fine_Tuning\\Gemme3-Unsloth\\.venv\\Lib\\site-packages\\triton\\knobs.py:212: UserWarning: Failed to find nvdisasm.exe\n",
      "  warnings.warn(f\"Failed to find {binary}\")\n",
      "c:\\Users\\Romaiz\\Documents\\Python_ML\\RL\\Fine_Tuning\\Gemme3-Unsloth\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "W1125 16:14:34.769000 31864 Lib\\site-packages\\torch\\distributed\\elastic\\multiprocessing\\redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a359d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=\"google/gemma-3-1b-it\",\n",
    "    max_seq_length=1024,\n",
    "    load_in_4bit=True,\n",
    "    load_in_8bit=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fee2cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    finetune_vision_layers=False,\n",
    "    finetune_language_layers=True,\n",
    "    finetune_attention_modules=True,\n",
    "    finetune_mlp_modules=True,\n",
    "    r=16,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3047\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0261f788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"madrylab/gsm8k-platinum\", \"main\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5377de3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d25764",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83523514",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee982d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_hash_answer(text):\n",
    "    if \"####\" not in text: return None\n",
    "    return text.split(\"####\")[1].strip()\n",
    "extract_hash_answer(dataset[0][\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516c0bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning = \"<reasoning>\"\n",
    "answer = \"<answer>\"\n",
    "\n",
    "system_prompt = \\\n",
    "f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your working out.\n",
    "Place it between {reasoning} and {reasoning}.\n",
    "Then, provide your solution between {answer}{answer}.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0dd5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(lambda x: {\n",
    "    \"prompt\" : [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\",   \"content\": x[\"question\"]},\n",
    "    ],\n",
    "    \"answer\": extract_hash_answer(x[\"answer\"]),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc652dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993252de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "match_format = re.compile(\n",
    "    rf\"^[\\s]{{0,}}\"\\\n",
    "    rf\"{reasoning}.+?{reasoning}.*?\"\\\n",
    "    rf\"{answer}(.+?){answer}\"\\\n",
    "    rf\"[\\s]{{0,}}$\",\n",
    "    flags = re.MULTILINE | re.DOTALL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab8a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_format.search(\"<reasoning> Here is my working out<reasoning>\"\\\n",
    "                    \"<answer>42 <answer>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714aadb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for output in completions:\n",
    "        score = 0\n",
    "        response = output[0][\"content\"]\n",
    "        if match_format.search(response) is not None:\n",
    "            score += 3\n",
    "            scores.append(score)\n",
    "        else:\n",
    "            scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246e0123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_format_partially(completions, **kwargs):\n",
    "    scores = []\n",
    "    for output in completions:\n",
    "        score = 0\n",
    "        response = output[0][\"content\"]\n",
    "        score += 1 if response.count(reasoning) == 2 else -1\n",
    "        score += 1 if response.count(answer) == 2 else -1\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c494aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer_correctness(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [output[0][\"content\"] for output in completions]\n",
    "\n",
    "    extracted_responses = [\n",
    "        guess.group(1) if (guess := match_format.search(r)) is not None else None for r in responses\n",
    "    ]\n",
    "\n",
    "    scores = []\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        score = 0\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        if guess == true_answer:\n",
    "            score += 3\n",
    "        elif guess.strip() == true_answer.strip():\n",
    "            score += 1.5\n",
    "        else:\n",
    "            try:\n",
    "                ratio = float(guess) / float(true_answer)\n",
    "                if ratio >= 0.9 and ratio <= 1.1:\n",
    "                    score += 0.5\n",
    "                elif ratio >= 0.8 and ratio <= 1.2:\n",
    "                    score += 0.25\n",
    "                else:\n",
    "                    score -= 1\n",
    "            except:\n",
    "                score -= 1\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f961ae91",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_numbers = re.compile(\n",
    "    rf\"{answer}.*?([\\d\\d.]{{1,}})\",\n",
    "    flags= re.MULTILINE | re.DOTALL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3023e869",
   "metadata": {},
   "outputs": [],
   "source": [
    "match_numbers.findall(\"<answer> The answer is 42.0 <answer>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0074c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    question = prompts[0][-1][\"content\"]\n",
    "    responses = [output[0][\"content\"] for output in completions]\n",
    "    \n",
    "    extracted_responses = [guess.group(1) if (guess := match_numbers.search(r)) is not None else None for r in responses]\n",
    "\n",
    "    scores = []\n",
    "    print(\"*\"*5, f\"\\nAnswer: {answer[0]}\", f\"\\nExtracted: {extracted_responses[0]}\")\n",
    "    for guess, true_answer in zip(extracted_responses, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0)\n",
    "            continue\n",
    "        try:\n",
    "            true_answer = float(true_answer)\n",
    "            guess = float(guess)\n",
    "            scores.append(1.5 if guess == true_answer else 0.0)\n",
    "        except:\n",
    "            scores.append(0.0)\n",
    "            continue\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d6006c",
   "metadata": {},
   "source": [
    "TRAINING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f2ada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6507e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(\n",
    "    learning_rate= 5e-6,\n",
    "    adam_beta1= 0.9,\n",
    "    adam_beta2= 0.99,\n",
    "    weight_decay= 0.1,\n",
    "    warmup_ratio= 0.1,\n",
    "    lr_scheduler_type= \"cosine\",\n",
    "    optim= \"adamw_8bit\",\n",
    "    logging_steps=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_generations=8,\n",
    "    max_prompt_length=256,\n",
    "    max_completion_length=768,\n",
    "    max_steps=50,\n",
    "    save_steps=50,\n",
    "    max_grad_norm=0.1,\n",
    "    report_to= \"wandb\",\n",
    "    output_dir= \"outputs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa47f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model= model,\n",
    "    processing_class= tokenizer,\n",
    "    reward_funcs= [\n",
    "        match_format_exactly,\n",
    "        match_format_partially,\n",
    "        check_answer_correctness,\n",
    "        check_numbers\n",
    "    ],\n",
    "    args= training_args,\n",
    "    train_dataset= dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e481ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc89f02-91fd-4852-9bb9-c2fd768ce4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"gemma-3-1b-finetuned\")  # Local saving\n",
    "tokenizer.save_pretrained(\"gemma-3-1b-finetuned\")  # Local saving\n",
    "model.push_to_hub(\"Romaiz_Dabeer/gemma-3-1b-finetuned-unsloth\") # Online saving\n",
    "tokenizer.push_to_hub(\"Romaiz_Dabeer/gemma-3\") # Online saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612402a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained_merged(\"gemma-3-1b-finetuned-unsloth\", tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7fe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub_merged(\n",
    "        \"Romaiz_Dabeer/gemma-3-finetune-finetuned-unsloth\", tokenizer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c30da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub_gguf(\n",
    "        \"HF_ACCOUNT/gemma-finetune-gguf\",\n",
    "        tokenizer,\n",
    "        quantization_method = \"Q8_0\", # Only Q8_0, BF16, F16 supported\n",
    "        token = \"hf_...\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9723504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
